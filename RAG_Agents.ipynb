{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4Z5FENcPorG",
        "outputId": "40d1fb3b-9c11-4ac3-9a9f-9ac9101f8a28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m943.5/943.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m577.4/577.4 kB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m221.5/221.5 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.0/143.0 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.7/14.7 MB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.1/139.1 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.0/82.0 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.8/81.8 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for deeplake (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q llama-index==0.9.14.post3 deeplake==3.8.8 openai==1.3.8 cohere==4.37"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = '<YOUR_OPENAI_API_KEY>'\n",
        "os.environ['ACTIVELOOP_TOKEN'] = '<YOUR_ACTIVELOOP_API_KEY>'"
      ],
      "metadata": {
        "id": "51Y1aB-eP8Y_"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%mkdir -p  './data/1k/'\n",
        "!wget 'https://github.com/idontcalculate/data-repo/blob/main/machine_to_end_war.txt' -O './data/1k/tesla.txt'\n",
        "!wget 'https://github.com/idontcalculate/data-repo/blob/main/prodigal_chapter10.txt' -O './data/1k/web.txt'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ld3KNW3aQB4L",
        "outputId": "b27ec96d-12af-4087-a6f0-97b15e07859d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-27 12:32:45--  https://github.com/idontcalculate/data-repo/blob/main/machine_to_end_war.txt\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 18608 (18K) [text/plain]\n",
            "Saving to: ‘./data/1k/tesla.txt’\n",
            "\n",
            "\r./data/1k/tesla.txt   0%[                    ]       0  --.-KB/s               \r./data/1k/tesla.txt 100%[===================>]  18.17K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2023-12-27 12:32:45 (728 KB/s) - ‘./data/1k/tesla.txt’ saved [18608/18608]\n",
            "\n",
            "--2023-12-27 12:32:45--  https://github.com/idontcalculate/data-repo/blob/main/prodigal_chapter10.txt\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 28208 (28K) [text/plain]\n",
            "Saving to: ‘./data/1k/web.txt’\n",
            "\n",
            "./data/1k/web.txt   100%[===================>]  27.55K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2023-12-27 12:32:45 (1.09 MB/s) - ‘./data/1k/web.txt’ saved [28208/28208]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import SimpleDirectoryReader\n",
        "\n",
        "tesla_docs = SimpleDirectoryReader( input_files=[\"/content/data/1k/tesla.txt\"] ).load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFUSAbTLQsTO",
        "outputId": "22f8dd2b-afa0-4a49-e6e2-fce846653de5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/deeplake/util/check_latest_version.py:32: UserWarning: A newer version of deeplake (3.8.13) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.vector_stores import DeepLakeVectorStore\n",
        "\n",
        "my_activeloop_org_id = \"doukansurel\"\n",
        "my_activeloop_dataset_name = \"LlamaIndex_tesla_predictions\"\n",
        "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\""
      ],
      "metadata": {
        "id": "9zjkpJ3tRVZ7"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an index over the documnts\n",
        "\n",
        "vector_store = DeepLakeVectorStore(dataset_path=dataset_path, overwrite=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMxwlUIDRfRa",
        "outputId": "fc20a528-78aa-4dac-86bd-2fe5c1d1b825"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your Deep Lake dataset has been successfully created!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.storage.storage_context import StorageContext\n",
        "from llama_index import VectorStoreIndex\n",
        "\n",
        "storage_context  = StorageContext.from_defaults(vector_store=vector_store)\n",
        "tesla_index = VectorStoreIndex.from_documents(tesla_docs,storage_context=storage_context)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4hgUjvTRgfi",
        "outputId": "0203b24d-f399-47b3-f21a-df66fb9e8370"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploading data to deeplake dataset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [00:00<00:00,  7.32it/s]\n",
            "|"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset(path='hub://doukansurel/LlamaIndex_tesla_predictions', tensors=['text', 'metadata', 'embedding', 'id'])\n",
            "\n",
            "  tensor      htype      shape     dtype  compression\n",
            "  -------    -------    -------   -------  ------- \n",
            "   text       text      (5, 1)      str     None   \n",
            " metadata     json      (5, 1)      str     None   \n",
            " embedding  embedding  (5, 1536)  float32   None   \n",
            "    id        text      (5, 1)      str     None   \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r \r"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Store Index Local\n",
        "webtext_docs = SimpleDirectoryReader(input_files=[\"/content/data/1k/web.txt\"]).load_data()"
      ],
      "metadata": {
        "id": "EPereKyoR4VX"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  # Try to load the index if it is already calculated\n",
        "  storage_context = StorageContext.from_defaults( persist_dir=\"/content/storage/webtext\" )\n",
        "  webtext_index = load_index_from_storage(storage_context)\n",
        "  print(\"Loaded the pre-computed index.\")\n",
        "except:\n",
        "  # Otherwise, generate the indexes\n",
        "  webtext_index = VectorStoreIndex.from_documents(webtext_docs)\n",
        "  webtext_index.storage_context.persist(persist_dir=\"/content/storage/webtext\")\n",
        "  print(\"Generated the index.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Id_e83WSW2h",
        "outputId": "27fbe7a2-2878-47e0-8454-e23ca24d67b6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated the index.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Query Engine"
      ],
      "metadata": {
        "id": "eLZLphmcSmpP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tesla_engine = tesla_index.as_query_engine(similarity_top_k=3)\n",
        "webtext_engine = webtext_index.as_query_engine(similarity_top_k=3)"
      ],
      "metadata": {
        "id": "x_F_C242SnCL"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.tools import QueryEngineTool, ToolMetadata\n",
        "\n",
        "query_engine_tools = [\n",
        "    QueryEngineTool(\n",
        "        query_engine=tesla_engine,\n",
        "        metadata = ToolMetadata(\n",
        "            name=\"tesla_1k\",\n",
        "            description=(\n",
        "                 \"Provides information about Tesla's statements that refers to future times and predictions. \"\n",
        "                \"Use a detailed plain text question as input to the tool.\"\n",
        "            ),\n",
        "        ),\n",
        "\n",
        "    ),\n",
        "\n",
        "  QueryEngineTool(\n",
        "      query_engine=webtext_engine,\n",
        "      metadata=ToolMetadata(\n",
        "          name=\"webtext_1k\",\n",
        "          description=(\n",
        "                \"Provides information about tesla's life and biographical data. \"\n",
        "                \"Use a detailed plain text question as input to the tool.\"\n",
        "            ),\n",
        "      ),\n",
        "  ),\n",
        "\n",
        "]"
      ],
      "metadata": {
        "id": "FxbORXn5TA8T"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "İşte mevcut sistemimizin açık bir şematik temsili. Sorgu motoru üstte tasvir edilmiştir ve bu da onun her şeyi düzenleyen birincil araç rolünü belirtir. Veri kaynakları ile nihai cevabı formüle etme süreci arasında merkezi bir konuma sahiptir. Önerilen sorular ile bunların yanıtları arasında bir köprü görevi görür. LlamaIndex ile temel RAG mekanizmasını kurduktan sonraki adım bir aracının entegrasyonudur. Bu ekleme, geri alma sisteminin kolay test edilmesini sağlar. Temel işlevsellik test edilip doğrulandıktan sonra sistem tasarımı iyileştirmeleri ve özellik geliştirmeleri ekleyebiliriz."
      ],
      "metadata": {
        "id": "I9luWAGGTyFV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: The Agent"
      ],
      "metadata": {
        "id": "paMC4zqZT0Nn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.agent import OpenAIAgent\n",
        "agent =OpenAIAgent.from_tools(query_engine_tools,verbose=True)\n"
      ],
      "metadata": {
        "id": "dfgL2lVeTyem"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Artık temsilcimize sahip olduğumuza göre, aracının girdileri (sorular veya istemler gibi) alabileceği, bunları işleyebileceği ve yanıtları geri döndürebileceği etkileşimli bir sohbet arayüzü (REPL, Okuma-Değerlendirme-Yazdırma Döngüsü) çalıştırabiliriz; bu da onu konuşmaya dayalı bir aracı yapabilir bir diyalog veya sohbet oturumunu yönetme."
      ],
      "metadata": {
        "id": "xJ3baJeQUW9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent.chat_repl()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTIjLTN2ULIG",
        "outputId": "1eb03088-a1bb-4ce3-befd-bf2920563ccf"
      },
      "execution_count": 21,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===== Entering Chat REPL =====\n",
            "Type \"exit\" to exit.\n",
            "\n",
            "Human: What influenced Nikola Tesla to become an inventor?\n",
            "STARTING TURN 1\n",
            "---------------\n",
            "\n",
            "=== Calling Function ===\n",
            "Calling function: webtext_1k with args: {\n",
            "\"input\": \"What influenced Nikola Tesla to become an inventor?\"\n",
            "}\n",
            "Got output: Nikola Tesla was influenced to become an inventor by his observations and studies of mechanical vibrations. He noticed that objects responded differently to vibrations and believed that resonance conditions could produce effects of tremendous magnitude on physical objects. This led him to explore the field of mechanical vibrations and develop inventions based on this principle.\n",
            "========================\n",
            "\n",
            "STARTING TURN 2\n",
            "---------------\n",
            "\n",
            "Assistant: Nikola Tesla was influenced to become an inventor by his observations and studies of mechanical vibrations. He noticed that objects responded differently to vibrations and believed that resonance conditions could produce effects of tremendous magnitude on physical objects. This led him to explore the field of mechanical vibrations and develop inventions based on this principle.\n",
            "\n",
            "Human: Nikola Tesla was influenced to become an inventor by his studies of mechanical vibrations. He observed the selective response of objects to vibrations and realized the potential for producing effects of tremendous magnitude on physical objects. This led him to pursue research in the field of high-frequency and high-potential currents, which eventually resulted in his groundbreaking inventions.\n",
            "STARTING TURN 1\n",
            "---------------\n",
            "\n",
            "Assistant: Yes, you are correct. Nikola Tesla was indeed influenced to become an inventor by his studies of mechanical vibrations. He observed the selective response of objects to vibrations and realized the potential for producing effects of tremendous magnitude on physical objects. This led him to pursue research in the field of high-frequency and high-potential currents, which eventually resulted in his groundbreaking inventions.\n",
            "\n",
            "Human: exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def multiply(a: int, b: int) -> int:\n",
        "    \"\"\"Multiply two integers and returns the result integer\"\"\"\n",
        "    return a * b\n",
        "\n",
        "\n",
        "def add(a: int, b: int) -> int:\n",
        "    \"\"\"Add two integers and returns the result integer\"\"\"\n",
        "    return a + b\n",
        "\n",
        "from llama_index.tools import FunctionTool\n",
        "\n",
        "multiply_tool = FunctionTool.from_defaults(fn=multiply, name=\"multiply\")\n",
        "add_tool = FunctionTool.from_defaults(fn=add, name=\"add\")\n",
        "\n",
        "all_tools = [multiply_tool, add_tool]"
      ],
      "metadata": {
        "id": "5kncN2bvVBBc"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yukarıdaki kod 'ekleme' ve 'çarpma' başlıklı iki işlev oluşturur. Bu kurulumda, giriş argümanları için veri türlerini (a:int, b:int), işlevin dönüş türünü (->int) ve üçlü tırnak işaretleri içinde sağlanan işlevin amacının kısa bir açıklamasını belirtmek çok önemlidir. işlev adının altında. Bu ayrıntılar, daha sonra aracı tarafından kullanılabilecek işlevin bir açıklamasını oluşturmak için FunctionTool sınıfının .from_defaults() yöntemi tarafından kullanılacaktır. Son değişken mevcut tüm araçların bir listesini içerir. Bu araçlar, VectorStoreIndex'i birden fazla olası araca bağlayan bir sarmalayıcı sınıf olan ObjectIndex'i oluşturmak için kullanılabilir. Başlangıçta, araç uygulamalarını düğümlere dönüştürmek ve ardından her şeyi birbirine bağlamak için SimpleToolNodeMapping aracını kullanmak gerekir."
      ],
      "metadata": {
        "id": "UIigYrQKVVKn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import VectorStoreIndex\n",
        "from llama_index.objects import ObjectIndex, SimpleToolNodeMapping\n",
        "\n",
        "tool_mapping = SimpleToolNodeMapping.from_objects(all_tools)\n",
        "onj_index = ObjectIndex.from_objects(\n",
        "    all_tools,\n",
        "    tool_mapping,\n",
        "    VectorStoreIndex,\n",
        ")"
      ],
      "metadata": {
        "id": "JqcR229yVVip"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bu uygulamaya herhangi bir veri kaynağı dahil etmediğimizi unutmayın. Büyük Dil Modellerinin yeteneklerini ek araçlarla geliştirmeyi hedeflediğimiz için bu yaklaşım kasıtlıdır. Bir sonraki kod bloğunda tanımlı nesne indeksini alıcı olarak kullandığımızı göreceksiniz! Bu, LlamaIndex çerçevesinde özel işlevlerin ek veri kaynakları olarak ele alındığı anlamına gelir. Bu nedenle, aracı nesnesini FnRetrieverOpenAIAgent sınıfını kullanarak tanımlıyoruz."
      ],
      "metadata": {
        "id": "CodPFH1oV5Ua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.agent import FnRetrieverOpenAIAgent\n",
        "\n",
        "agent = FnRetrieverOpenAIAgent.from_retriever(\n",
        "    onj_index.as_retriever(),verbose = True\n",
        ")"
      ],
      "metadata": {
        "id": "OpxRw7baV5tu"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.chat(\"What's 12 multiplied by 22? Make sure to use Tools\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmxPQCgHWIqf",
        "outputId": "e5a64819-f216-4784-8e30-4729355d2ea0"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STARTING TURN 1\n",
            "---------------\n",
            "\n",
            "=== Calling Function ===\n",
            "Calling function: multiply with args: {\n",
            "  \"a\": 12,\n",
            "  \"b\": 22\n",
            "}\n",
            "Got output: 264\n",
            "========================\n",
            "\n",
            "STARTING TURN 2\n",
            "---------------\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AgentChatResponse(response='12 multiplied by 22 is 264.', sources=[ToolOutput(content='264', tool_name='multiply', raw_input={'args': (), 'kwargs': {'a': 12, 'b': 22}}, raw_output=264)], source_nodes=[])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Önceki örnekte, istemde aracının araçları kullanması gerektiğini belirtmiştik. Ek olarak, aracıyı belirli araçları kullanmaya açıkça yönlendirmek için tool_choice argümanını kullanmak veya aracının karar vermesine izin vermek için auto anahtar sözcüğünü kullanmak mümkündür."
      ],
      "metadata": {
        "id": "T-j1qtzCWWL1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = agent.chat( \"What is 5 + 2?\", tool_choice=\"add\" )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyOhNALuWaMk",
        "outputId": "ee023f70-85b1-48c5-94e3-2f3839ed0ccf"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STARTING TURN 1\n",
            "---------------\n",
            "\n",
            "=== Calling Function ===\n",
            "Calling function: add with args: {\n",
            "  \"a\": 5,\n",
            "  \"b\": 2\n",
            "}\n",
            "Got output: 7\n",
            "========================\n",
            "\n",
            "STARTING TURN 2\n",
            "---------------\n",
            "\n"
          ]
        }
      ]
    }
  ]
}